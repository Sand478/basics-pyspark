{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Spark**:\n",
    "- Фреймворк для распределённой обработки данных\n",
    "- Входит в экосистему Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Что значит распределённая обработка данных? Когда она нужна?\n",
    "- данных много\n",
    "- параллельность\n",
    "- масштабирование\n",
    "\n",
    "\n",
    "#### 2. Что такое Hadoop? Каковы его особенности?\n",
    "- экосистема = набор инструментов\n",
    "- HDFS\n",
    "- оркестратор=YARN\n",
    "\n",
    "\n",
    "#### 3. Что такое MapReduce? Как он работает?\n",
    "- алгоритм или способ обработки\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преимущества Spark по сравнению с MapReduce:\n",
    "* Сам простраивает выполнение – не нужно специально писать mapper'ы и reducer'ы\n",
    "* Максимально выполняет обработку в памяти, не сбрасывая на диск\n",
    "* Много оптимизаций\n",
    "* Легко делать итеративные алгоритмы – часто это машинное обучение\n",
    "\n",
    "Доступные языки: Scala, Java, **Python**, R, SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Три уровня API к данным:\n",
    "* RDD – Resilient Data Set, неизменяемая коллекция данных (массив объектов)\n",
    "* **DataFrame** – абстракция над RDD, позволяет выполнять произвольный SQL (таблица в БД)\n",
    "* Dataset – строго типизированная версия DataFrame, отсутствует за пределами Scala и Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы пользоваться DataFrame API, нужно создать Spark-сессию\n",
    "\n",
    "`spark.stop()` - чтобы пересоздать заново, сначала нужно остановить предыдущую. Spark-сессия – это [синглтон](https://webdevblog.ru/realizaciya-shablona-singleton-v-python/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/12 11:32:32 WARN Utils: Your hostname, spark-server resolves to a loopback address: 127.0.1.1; using 10.0.0.31 instead (on interface eth0)\n",
      "23/01/12 11:32:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/12 11:32:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.31:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Apache Spark Lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1e6c1162c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Apache Spark Lecture\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Откуда можно взять данные?**\n",
    "* из локальной коллекции\n",
    "* из распределённой файловой системы (HDFS, S3, etc)\n",
    "* из базы данных (PostgreSQL, Cassandra, etc)\n",
    "* откуда угодно, если написать коннектор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала создадим его из локальной коллекции структур:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "data = [\n",
    "    [\"user1\",  datetime.datetime(2022, 6, 7),   1234556, 1,  567.8],\n",
    "    [\"user2\",  datetime.datetime(2022, 6, 8),   2345633, 2,  1276.0],\n",
    "    [\"user3\",  datetime.datetime(2022, 10, 11), 3687665, 10, 1053.0],\n",
    "    [\"user5\",  datetime.datetime(2022, 10, 11), 3687665, 2,  210.6],\n",
    "    [\"user6\",  datetime.datetime(2022, 10, 12), 5348776, 5,  2000.0],\n",
    "    [\"user7\",  datetime.datetime(2022, 12, 1),  2345765, 8,  186.0],\n",
    "    [\"user8\",  datetime.datetime(2021, 10, 11), 2369867, 3,  900.0],\n",
    "    [\"user9\",  datetime.datetime(2021, 2, 3),   1234556, 1,  567.8],\n",
    "    [\"user10\", datetime.datetime(2021, 5, 10),  2563574, 1,  1050.0],\n",
    "    [\"user11\", datetime.datetime(2022, 10, 11), 2346354, 1,  400.0],\n",
    "    [\"user12\", datetime.datetime(2022, 6, 15),  8796467, 4,  2400.0],\n",
    "    [\"user13\", datetime.datetime(2022, 6, 15),  4573645, 10, 1050.0],\n",
    "    [\"user14\", datetime.datetime(2022, 9, 2),   2936764, 8,  64.0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: timestamp, _3: bigint, _4: bigint, _5: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У датафрейма есть схема: названия, типы и характеристики колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: timestamp (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      " |-- _4: long (nullable = true)\n",
      " |-- _5: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # выводит схему в поток вывода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Пример большей вложенности схемы\n",
    "spark.createDataFrame([\n",
    "    [[1, 2], 3],\n",
    "    [[4, 5], 6]\n",
    "]).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема – это объект типа [StructType](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html?highlight=structtype#pyspark.sql.types.StructType).\n",
    "Отдельные поля представлены объектами типа [StructField](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html?highlight=structtype#pyspark.sql.types.StructField)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_1,StringType,true),StructField(_2,TimestampType,true),StructField(_3,LongType,true),StructField(_4,LongType,true),StructField(_5,DoubleType,true)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2', '_3', '_4', '_5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание своей схемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user\", StringType(),    nullable = False),\n",
    "    StructField(\"date\", TimestampType(), nullable = False),\n",
    "    StructField(\"product_id\", LongType(), nullable = False),\n",
    "    StructField(\"quantity\", IntegerType(), nullable = False),\n",
    "    StructField(\"payment\", DoubleType(), nullable = False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = false)\n",
      " |-- date: timestamp (nullable = false)\n",
      " |-- product_id: long (nullable = false)\n",
      " |-- quantity: integer (nullable = false)\n",
      " |-- payment: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark следит за выполнением ограничения nullable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "field product_id: This field is not nullable, but got None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2022\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:675\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[1;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:700\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m    702\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:509\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 509\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    512\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:682\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m--> 682\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/types.py:1411\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj):\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 1411\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/types.py:1392\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1389\u001b[0m             new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of object (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) does not match with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1390\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of fields (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(obj), \u001b[38;5;28mlen\u001b[39m(verifiers))))\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 1392\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1394\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/types.py:1410\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj):\n\u001b[0;32m-> 1410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mverify_nullability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1411\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/types.py:1280\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_nullability\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis field is not nullable, but got None\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: field product_id: This field is not nullable, but got None"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([\n",
    "    [\"user1\",  datetime.datetime(2022, 6, 7),   None, 1,  None]\n",
    "], schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*О дополнительных способах создать DataFrame можно почитать [тут](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#DataFrame-Creation)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Просмотр содержимого"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод show() выводит часть датафрейма в поток вывода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----------+--------+-------+\n",
      "|  user|               date|product_id|quantity|payment|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "| user1|2022-06-07 00:00:00|   1234556|       1|  567.8|\n",
      "| user2|2022-06-08 00:00:00|   2345633|       2| 1276.0|\n",
      "| user3|2022-10-11 00:00:00|   3687665|      10| 1053.0|\n",
      "| user5|2022-10-11 00:00:00|   3687665|       2|  210.6|\n",
      "| user6|2022-10-12 00:00:00|   5348776|       5| 2000.0|\n",
      "| user7|2022-12-01 00:00:00|   2345765|       8|  186.0|\n",
      "| user8|2021-10-11 00:00:00|   2369867|       3|  900.0|\n",
      "| user9|2021-02-03 00:00:00|   1234556|       1|  567.8|\n",
      "|user10|2021-05-10 00:00:00|   2563574|       1| 1050.0|\n",
      "|user11|2022-10-11 00:00:00|   2346354|       1|  400.0|\n",
      "|user12|2022-06-15 00:00:00|   8796467|       4| 2400.0|\n",
      "|user13|2022-06-15 00:00:00|   4573645|      10| 1050.0|\n",
      "|user14|2022-09-02 00:00:00|   2936764|       8|   64.0|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+--------+-------+\n",
      "| user|               date|product_id|quantity|payment|\n",
      "+-----+-------------------+----------+--------+-------+\n",
      "|user1|2022-06-07 00:00:00|   1234556|       1|  567.8|\n",
      "|user2|2022-06-08 00:00:00|   2345633|       2| 1276.0|\n",
      "+-----+-------------------+----------+--------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2) # Вывести только 2 строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " user       | user1               \n",
      " date       | 2022-06-07 00:00:00 \n",
      " product_id | 1234556             \n",
      " quantity   | 1                   \n",
      " payment    | 567.8               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True) # Вертикальное отображение: полезно, когда колонок слишком много"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что если значение слишком длинное?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = spark.createDataFrame([\n",
    "    [\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  _1|\n",
      "+--------------------+\n",
      "|AAAAAAAAAAAAAAAAA...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "long_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_1                                                                                                                                      |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "long_df.show(truncate=False) # Выводить значения полностью. Полезно для визуального сравнения длинных значений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запись и чтение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем наш датасет. Для этого используем [DataFrameWriter](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html?highlight=dataframewriter#pyspark.sql.DataFrameWriter), который можно получить методом write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "writer = df.write # отдельная переменная для наглядности\n",
    "writer.parquet(\"/tmp/data/test_df\") # записать в формате Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-80b8d9c2-ea17-4645-a69b-567f41572de7-c000.snappy.parquet  _SUCCESS\n",
      "part-00001-80b8d9c2-ea17-4645-a69b-567f41572de7-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "ls /tmp/data/test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более развёрнутый вариант:\n",
    "- **format**: название формата\n",
    "- **mode**: поведение в том случае, если данные существуют \n",
    "    - *append*: добавить в конец\n",
    "    - *overwrite*: перезаписать\n",
    "    - *error*: упасть с ошибкой\n",
    "    - *ignore*: игнорировать запись\n",
    "- **save**: запускает запись по указанному пути    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"/tmp/data/test_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь прочитаем с помощью [DataFrameReader](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html?highlight=dataframereader#pyspark.sql.DataFrameReader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = spark.read\n",
    "df = reader.parquet(\"/tmp/data/test_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user: string, date: timestamp, product_id: bigint, quantity: int, payment: double]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более развёрнутый вариант:\n",
    "- **format**: название формата\n",
    "- **schema**: принудительная схема датафрейма\n",
    "- **load**: индексирует файлы по указанному пути для чтения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").schema(schema).load(\"/tmp/data/test_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+--------+-------+\n",
      "| user|               date|product_id|quantity|payment|\n",
      "+-----+-------------------+----------+--------+-------+\n",
      "|user8|2021-10-11 00:00:00|   2369867|       3|  900.0|\n",
      "+-----+-------------------+----------+--------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Список (неполный) поддерживаемых источников данных\n",
    "+ Файлы:\n",
    "    - json\n",
    "    - text\n",
    "    - csv\n",
    "    - orc\n",
    "    - parquet\n",
    "    - delta\n",
    "+ Базы данных\n",
    "    - elasticsearch\n",
    "    - cassandra\n",
    "    - jdbc\n",
    "    - hive\n",
    "    - redis\n",
    "    - mongo\n",
    "+ Брокеры сообщений\n",
    "    - kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение и запись CSV-файлов может быть сложнее: нужно следить за разделителем и другими нюансами и указывать их в методе **options** (есть и у DataFrameReader, и у DataFrameWriter).\n",
    "В документации Spark есть хороший [гайд](https://spark.apache.org/docs/latest/sql-data-sources-csv.html) того, как с такими файлами обращаться.                                                                                                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение и запись с партиционированием по колонке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датафреймы можно \"раскладывать\" по значению одной или нескольких колонок. Это помогает в дальнейшем уменьшить количество читаемых данных, если нужны записи в определённом диапазоне значений этих колонок (см далее блок **FILTER/WHERE**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    [\"2022-10-30\", \"hjksjfgh\", 345345],\n",
    "    [\"2022-10-31\", \"sdfsdf\", 456456]    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычная запись без партиционирования будет выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.parquet(\"/tmp/data/df1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: bigint]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-4766be33-e3fc-4629-8b5b-c019c8bb844f-c000.snappy.parquet  _SUCCESS\n",
      "part-00001-4766be33-e3fc-4629-8b5b-c019c8bb844f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "ls /tmp/data/df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью метода **DataFrameWriter.partitionBy** можно партиционировать по значению одной или нескольких колонок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.write.partitionBy(\"_1\").parquet(\"/tmp/data/df2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m'_1=2022-10-30'\u001b[0m/  \u001b[01;34m'_1=2022-10-31'\u001b[0m/   _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "ls /tmp/data/df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# порядок колонок важен: он определяет вложенность поддиректорий\n",
    "df1.write.partitionBy(\"_1\", \"_2\").parquet(\"/tmp/data/df3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-2e3425bf-a033-4960-af99-822a3f3f1ecc.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "ls /tmp/data/df3/_1=2022-10-30/_2=hjksjfgh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читать можно как родительскую директорию, так и поддиректории. Чтение поддиректории – автоматическая фильтрация по значению соответствующей колонки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+\n",
      "|      _2|    _3|        _1|\n",
      "+--------+------+----------+\n",
      "|hjksjfgh|345345|2022-10-30|\n",
      "|  sdfsdf|456456|2022-10-31|\n",
      "+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/tmp/data/df2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|      _2|    _3|\n",
      "+--------+------+\n",
      "|hjksjfgh|345345|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/tmp/data/df2/_1=2022-10-30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью DataFrameReader также можно читать несколько файлов за один раз. Если схемы соответствующих датафреймов различаются, см раздел [Schema merging](https://spark.apache.org/docs/2.3.2/sql-programming-guide.html#schema-merging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|      _2|    _3|\n",
      "+--------+------+\n",
      "|hjksjfgh|345345|\n",
      "|  sdfsdf|456456|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/tmp/data/df2/_1=2022-10-30\", \"/tmp/data/df2/_1=2022-10-31\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые SQL-операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELECT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|  user|product_id|\n",
      "+------+----------+\n",
      "| user8|   2369867|\n",
      "| user9|   1234556|\n",
      "|user10|   2563574|\n",
      "+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"user\", \"product_id\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обращения к колонке и выполнения над ней операций используется класс [Column](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html?highlight=column#pyspark.sql.Column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'user'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|newname|\n",
      "+-------+\n",
      "|  user8|\n",
      "+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.user.alias(\"newname\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|newname|\n",
      "+-------+\n",
      "|  user8|\n",
      "+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df.select(F.col(\"user\").alias(\"newname\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод explain() показывает план выполнения запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [user#119 AS newname#238]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [user#119] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/test_df], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.user.alias(\"newname\")).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Parquet – колоночный формат. Он позволяет читать только отдельные колонки. Spark этим пользуется! <br/>\n",
    "Такие оптимизации выполняются мощным компонентом [Catalyst](https://www.unraveldata.com/resources/catalyst-analyst-a-deep-dive-into-sparks-optimizer/). Они минимизируют передачу данных по сети и загрузку их в память."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [user#119 AS newname#240]\n",
      "+- Relation [user#119,date#120,product_id#121L,quantity#122,payment#123] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "newname: string\n",
      "Project [user#119 AS newname#240]\n",
      "+- Relation [user#119,date#120,product_id#121L,quantity#122,payment#123] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [user#119 AS newname#240]\n",
      "+- Relation [user#119,date#120,product_id#121L,quantity#122,payment#123] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [user#119 AS newname#240]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [user#119] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/test_df], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.user.alias(\"newname\")).explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ORDER BY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----------+--------+-------+\n",
      "|  user|               date|product_id|quantity|payment|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "| user1|2022-06-07 00:00:00|   1234556|       1|  567.8|\n",
      "| user9|2021-02-03 00:00:00|   1234556|       1|  567.8|\n",
      "| user2|2022-06-08 00:00:00|   2345633|       2| 1276.0|\n",
      "| user7|2022-12-01 00:00:00|   2345765|       8|  186.0|\n",
      "|user11|2022-10-11 00:00:00|   2346354|       1|  400.0|\n",
      "| user8|2021-10-11 00:00:00|   2369867|       3|  900.0|\n",
      "|user10|2021-05-10 00:00:00|   2563574|       1| 1050.0|\n",
      "|user14|2022-09-02 00:00:00|   2936764|       8|   64.0|\n",
      "| user5|2022-10-11 00:00:00|   3687665|       2|  210.6|\n",
      "| user3|2022-10-11 00:00:00|   3687665|      10| 1053.0|\n",
      "|user13|2022-06-15 00:00:00|   4573645|      10| 1050.0|\n",
      "| user6|2022-10-12 00:00:00|   5348776|       5| 2000.0|\n",
      "|user12|2022-06-15 00:00:00|   8796467|       4| 2400.0|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"product_id\").show() # по дефолту asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----------+--------+-------+\n",
      "|  user|               date|product_id|quantity|payment|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "| user9|2021-02-03 00:00:00|   1234556|       1|  567.8|\n",
      "| user1|2022-06-07 00:00:00|   1234556|       1|  567.8|\n",
      "| user2|2022-06-08 00:00:00|   2345633|       2| 1276.0|\n",
      "| user7|2022-12-01 00:00:00|   2345765|       8|  186.0|\n",
      "|user11|2022-10-11 00:00:00|   2346354|       1|  400.0|\n",
      "| user8|2021-10-11 00:00:00|   2369867|       3|  900.0|\n",
      "|user10|2021-05-10 00:00:00|   2563574|       1| 1050.0|\n",
      "|user14|2022-09-02 00:00:00|   2936764|       8|   64.0|\n",
      "| user5|2022-10-11 00:00:00|   3687665|       2|  210.6|\n",
      "| user3|2022-10-11 00:00:00|   3687665|      10| 1053.0|\n",
      "|user13|2022-06-15 00:00:00|   4573645|      10| 1050.0|\n",
      "| user6|2022-10-12 00:00:00|   5348776|       5| 2000.0|\n",
      "|user12|2022-06-15 00:00:00|   8796467|       4| 2400.0|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# эквивалентно\n",
    "df.select(\"*\").orderBy(\"product_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----------+--------+-------+\n",
      "|  user|               date|product_id|quantity|payment|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "|user12|2022-06-15 00:00:00|   8796467|       4| 2400.0|\n",
      "| user6|2022-10-12 00:00:00|   5348776|       5| 2000.0|\n",
      "|user13|2022-06-15 00:00:00|   4573645|      10| 1050.0|\n",
      "| user3|2022-10-11 00:00:00|   3687665|      10| 1053.0|\n",
      "| user5|2022-10-11 00:00:00|   3687665|       2|  210.6|\n",
      "|user14|2022-09-02 00:00:00|   2936764|       8|   64.0|\n",
      "|user10|2021-05-10 00:00:00|   2563574|       1| 1050.0|\n",
      "| user8|2021-10-11 00:00:00|   2369867|       3|  900.0|\n",
      "|user11|2022-10-11 00:00:00|   2346354|       1|  400.0|\n",
      "| user7|2022-12-01 00:00:00|   2345765|       8|  186.0|\n",
      "| user2|2022-06-08 00:00:00|   2345633|       2| 1276.0|\n",
      "| user1|2022-06-07 00:00:00|   1234556|       1|  567.8|\n",
      "| user9|2021-02-03 00:00:00|   1234556|       1|  567.8|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.product_id.desc()).show() # asc/desc можно вызвать у Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTER/WHERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----------+--------+-------+\n",
      "|  user|               date|product_id|quantity|payment|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "| user8|2021-10-11 00:00:00|   2369867|       3|  900.0|\n",
      "| user9|2021-02-03 00:00:00|   1234556|       1|  567.8|\n",
      "|user10|2021-05-10 00:00:00|   2563574|       1| 1050.0|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"payment > 100\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----------+--------+-------+\n",
      "|  user|               date|product_id|quantity|payment|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "| user8|2021-10-11 00:00:00|   2369867|       3|  900.0|\n",
      "| user9|2021-02-03 00:00:00|   1234556|       1|  567.8|\n",
      "|user10|2021-05-10 00:00:00|   2563574|       1| 1050.0|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"payment > 100\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----------+--------+-------+\n",
      "|  user|               date|product_id|quantity|payment|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "| user8|2021-10-11 00:00:00|   2369867|       3|  900.0|\n",
      "| user9|2021-02-03 00:00:00|   1234556|       1|  567.8|\n",
      "|user10|2021-05-10 00:00:00|   2563574|       1| 1050.0|\n",
      "+------+-------------------+----------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.payment > 100).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(payment#123) AND (payment#123 > 100.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [user#119,date#120,product_id#121L,quantity#122,payment#123] Batched: true, DataFilters: [isnotnull(payment#123), (payment#123 > 100.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/test_df], PartitionFilters: [], PushedFilters: [IsNotNull(payment), GreaterThan(payment,100.0)], ReadSchema: struct<user:string,date:timestamp,product_id:bigint,quantity:int,payment:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.payment > 100).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Вместо того, чтобы фильтровать после загрузки, файл фильтруется при чтении. В память попадают только нужные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('payment > 100)\n",
      "+- Project [user#119, payment#123]\n",
      "   +- Relation [user#119,date#120,product_id#121L,quantity#122,payment#123] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "user: string, payment: double\n",
      "Filter (payment#123 > cast(100 as double))\n",
      "+- Project [user#119, payment#123]\n",
      "   +- Relation [user#119,date#120,product_id#121L,quantity#122,payment#123] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [user#119, payment#123]\n",
      "+- Filter (isnotnull(payment#123) AND (payment#123 > 100.0))\n",
      "   +- Relation [user#119,date#120,product_id#121L,quantity#122,payment#123] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(payment#123) AND (payment#123 > 100.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [user#119,payment#123] Batched: true, DataFilters: [isnotnull(payment#123), (payment#123 > 100.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/test_df], PartitionFilters: [], PushedFilters: [IsNotNull(payment), GreaterThan(payment,100.0)], ReadSchema: struct<user:string,payment:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"user\", \"payment\").filter(\"payment > 100\").explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Фильтр проталкивается как можно ближе к источнику данных, даже если был определён дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При фильтрации по колонке партиционирования будут читаться только файлы в поддиректориях с соответствующим значением колонки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [_2#375,_3#376L,_1#377] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/df2], PartitionFilters: [isnotnull(_1#377), (_1#377 = 2022-10-30)], PushedFilters: [], ReadSchema: struct<_2:string,_3:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/tmp/data/df2\").where(\"_1 = '2022-10-30'\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COUNT и DISTINCT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|   2346354|\n",
      "|   8796467|\n",
      "|   2936764|\n",
      "|   2563574|\n",
      "|   1234556|\n",
      "|   2369867|\n",
      "|   4573645|\n",
      "|   3687665|\n",
      "|   2345765|\n",
      "|   2345633|\n",
      "|   5348776|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"product_id\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"product_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'explain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m() \u001b[38;5;66;03m# у объекта int нет explain\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'explain'"
     ]
    }
   ],
   "source": [
    "df.select(\"product_id\").distinct().count().explain() # у объекта int нет explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#121L], functions=[])\n",
      "   +- Exchange hashpartitioning(product_id#121L, 200), ENSURE_REQUIREMENTS, [plan_id=645]\n",
      "      +- HashAggregate(keys=[product_id#121L], functions=[])\n",
      "         +- FileScan parquet [product_id#121L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/test_df], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"product_id\").distinct().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[product_id: bigint]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"product_id\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ленивое выполнение. Трансформации и действия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисления в Spark ленивые – происходят только тогда, когда нужен их результат.<br/>\n",
    "Все методы обработки данных над DataFrame делятся на **трансформации** и **действия**: <br/>\n",
    "- *Трансформации* добавляют новый пункт в план выполнения – вычислений не происходит<br/>\n",
    "- *Действия* запускают вычисление всего плана, что был до них"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос: какие методы из тех, что были выше, являются трансформациями, а какие – действиями?**<br/>\n",
    "*Трансформации:* distinct, filter/where, select, load, orderBy<br/>\n",
    "*Действия:* count, save, show, [explain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Очистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем датасет побольше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07-spark.ipynb         population_by_country_2020.csv\n",
      "airport-codes_csv.csv  \u001b[0m\u001b[01;34mtest\u001b[0m/\n",
      "demo.ipynb             \u001b[01;34mtest_2\u001b[0m/\n",
      "\u001b[01;34mparq\u001b[0m/                  wikipedia-iso-country-codes.csv\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+------------+---------+-----------+----------+--------------------+--------+---------+----------+--------------------+\n",
      "|  ident|          type|                name|elevation_ft|continent|iso_country|iso_region|        municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-------+--------------+--------------------+------------+---------+-----------+----------+--------------------+--------+---------+----------+--------------------+\n",
      "|   SIYK| small_airport|Fazenda Sorriso M...|        2605|       SA|         BR|     BR-GO|              Urutai|    SIYK|     null|      null|-48.2152786254882...|\n",
      "|   MI92| small_airport|  Lilienthal Airport|        1250|       NA|         US|     US-MI|       Iron Mountain|    MI92|     null|      MI92|-88.0981979370117...|\n",
      "|   MI80| small_airport|Wabasis Lake Airport|         892|       NA|         US|     US-MI|          Greenville|    MI80|     null|      MI80|-85.3992004394531...|\n",
      "|   SNXK| small_airport|Fazenda Lagoa do ...|         558|       SA|         BR|     BR-PA|SÃ£o FÃ©lix Do Xingu|    SNXK|     null|      null|-52.7691688537597...|\n",
      "|FR-0042|      heliport|Centre Hospitalie...|         781|       EU|         FR|    FR-BFC|               Dijon|    null|     null|      null|5.026944160461426...|\n",
      "|   SCPU| small_airport|      Peulla Airport|         900|       SA|         CL|     CL-LL|              Peulla|    SCPU|     null|      null|-72.0083312988281...|\n",
      "|   LZZI|medium_airport|     Å½ilina Airport|        1020|       EU|         SK|     SK-ZI|             Å½ilina|    LZZI|      ILZ|      null|18.6135005951, 49...|\n",
      "|   OR14| small_airport|     Juntura Airport|        3034|       NA|         US|     US-OR|             Juntura|    OR14|     null|      OR14|-118.064002990722...|\n",
      "|   SIVA|      heliport|Cereal Citrus Hel...|        3183|       SA|         BR|     BR-DF|           BrasÃ­lia|    SIVA|     null|      null|-47.3694458007999...|\n",
      "|   CT84|      heliport|Partyka Chevrolet...|          50|       NA|         US|     US-CT|              Hamden|    CT84|     null|      CT84|-72.9086990356445...|\n",
      "|MX-0979|      heliport|Plaza Insurgentes...|        7782|       NA|         MX|    MX-DIF|      Alvaro Obregon|    null|     null|       HPF|-99.187649, 19.35...|\n",
      "|SK-0058| small_airport|RimavskÃ¡ SeÄ Ai...|        null|       EU|         SK|    SK-U-A|                null|    null|     null|      null|20.2502328, 48.31...|\n",
      "|   0OA2|      heliport|Cleveland Clinic,...|         983|       NA|         US|     US-OH|              Medina|    0OA2|     null|      0OA2|-81.836745, 41.13...|\n",
      "|    SNQ| small_airport|San QuintÃ­n Mili...|          68|       NA|         MX|    MX-BCN|Military Camp Num...|    null|      SNQ|      null|  -115.9462, 30.5288|\n",
      "|KR-0127|      heliport|Geomsan-dong Helipad|        null|       AS|         KR|     KR-41|       Geomsan-ddong|    null|     null|      null|126.751521, 37.79612|\n",
      "|   CMF4| small_airport|Port Hope (Millso...|         525|       NA|         CA|     CA-ON|                null|    CMF4|     null|      CMF4|-78.428691, 43.98...|\n",
      "|MX-1216| small_airport|MontaÃ±as del Gua...|          23|       NA|         MX|    MX-TAM|      Villa Gonzalez|    null|     null|       MDG|-98.340972, 22.55...|\n",
      "|   KBED|medium_airport|Laurence G Hansco...|         133|       NA|         US|     US-MA|             Bedford|    KBED|      BED|       BED|-71.28900146, 42....|\n",
      "|   LOIJ| small_airport|St. Johann In Tir...|        2198|       EU|         AT|      AT-7| St. Johann In Tirol|    LOIJ|     null|      null|    12.4497, 47.5201|\n",
      "|   81MI|      heliport|Three Rivers Heal...|         818|       NA|         US|     US-MI|        Three Rivers|    81MI|     null|      81MI|-85.648708, 41.93...|\n",
      "+-------+--------------+--------------------+------------+---------+-----------+----------+--------------------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "codes1 = spark.read\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".csv(\"airport-codes_csv.csv\")\n",
    "\n",
    "codes1.repartition(5).write.mode(\"overwrite\").parquet(\"/tmp/data/airport_codes\")\n",
    "\n",
    "codes = spark.read.parquet(\"/tmp/data/airport_codes\")\n",
    "\n",
    "codes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dropDuplicates**: трансформация, удаляет строки с дублями <br/>\n",
    "По умолчанию dropDuplicates() = distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57421"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57421"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр *subset* задаёт список колонок для дедупликации. <br/>\n",
    "Возьмётся по одной *любой* записи на каждую уникальную комбинацию значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.dropDuplicates(subset=[\"type\", \"iso_country\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|              name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "| MI92|small_airport|Lilienthal Airport|        1250|       NA|         US|     US-MI|Iron Mountain|    MI92|     null|      MI92|-88.0981979370117...|\n",
      "+-----+-------------+------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "codes.dropDuplicates(subset=[\"type\", \"iso_country\"])\\\n",
    ".where(\"type = 'small_airport' and iso_country = 'US'\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**na**: возвращает вспомогательный объект с трансформациями для работы с пустыми значениями [DataFrameNaFunctions](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html?highlight=dataframenafunctions#pyspark.sql.DataFrameNaFunctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2787"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_any = codes.na.drop() # убрать строки, в которых *какая-либо* из колонок пустая\n",
    "drop_any.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57421"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_all = codes.na.drop(how='all') # убрать строки, в которых *все* колонки пустые\n",
    "drop_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "| KFDR|small_airport|Frederick Regiona...|        1258|       NA|         US|     US-OK|   Frederick|    KFDR|      FDR|       FDR|-98.98390198, 34....|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "codes\\\n",
    ".na.drop()\\\n",
    ".dropDuplicates(subset=[\"type\", \"iso_country\"])\\\n",
    ".where(\"type = 'small_airport' and iso_country = 'US'\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если нужно убрать пустые значения только по одной колонке?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9225"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.where(\"iata_code is not null\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+------------+---------+-----------+----------+--------------------+--------+---------+----------+--------------------+\n",
      "|ident|          type|                name|elevation_ft|continent|iso_country|iso_region|        municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+--------------+--------------------+------------+---------+-----------+----------+--------------------+--------+---------+----------+--------------------+\n",
      "| LZZI|medium_airport|     Å½ilina Airport|        1020|       EU|         SK|     SK-ZI|             Å½ilina|    LZZI|      ILZ|      null|18.6135005951, 49...|\n",
      "|  SNQ| small_airport|San QuintÃ­n Mili...|          68|       NA|         MX|    MX-BCN|Military Camp Num...|    null|      SNQ|      null|  -115.9462, 30.5288|\n",
      "| KBED|medium_airport|Laurence G Hansco...|         133|       NA|         US|     US-MA|             Bedford|    KBED|      BED|       BED|-71.28900146, 42....|\n",
      "+-----+--------------+--------------------+------------+---------+-----------+----------+--------------------+--------+---------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "codes.where(\"iata_code is not null\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Встроенные и пользовательские функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark обладает достаточно большим набором встроенных функций, доступных в [pyspark.sql.functions](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#module-pyspark.sql.functions)<br/>\n",
    "Все функции Spark принимают на вход и возвращают Column, а это значит, что вы можете совмещать функции вместе. <br/>\n",
    "**Функции и колонки в Spark могут быть созданы без привязки к конкретным данным и DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F # некоторые методы называются так же, как встроенные в Python функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'split(coordinates, , , -1) AS coords'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_coords = F.split(F.col(\"coordinates\"), pattern=\", \").alias(\"coords\") # возвращает массив \n",
    "split_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_coords = codes.select(F.col(\"name\"), split_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                name|              coords|\n",
      "+--------------------+--------------------+\n",
      "|Fazenda Sorriso M...|[-48.215278625488...|\n",
      "|  Lilienthal Airport|[-88.098197937011...|\n",
      "|Wabasis Lake Airport|[-85.399200439453...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_coords.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------------------------------------+\n",
      "|name                             |coords                                   |\n",
      "+---------------------------------+-----------------------------------------+\n",
      "|Fazenda Sorriso MetÃ¡lico Airport|[-48.21527862548828, -17.456666946411133]|\n",
      "|Lilienthal Airport               |[-88.09819793701172, 45.932701110839844] |\n",
      "|Wabasis Lake Airport             |[-85.39920043945312, 43.12839889526367]  |\n",
      "+---------------------------------+-----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_coords.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**withColumn**: трансформация, которая добавляет новую колонку, вычисляемую через Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_rads = name_coords.withColumn(\"coords_rad\", F.struct(\n",
    "    F.radians(F.col(\"coords\")[0]).alias(\"lat\"),\n",
    "    F.radians(F.col(\"coords\")[1]).alias(\"lon\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- coords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- coords_rad: struct (nullable = false)\n",
      " |    |-- lat: double (nullable = true)\n",
      " |    |-- lon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_rads.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------------------------------------+------------------------------------------+\n",
      "|name                             |coords                                   |coords_rad                                |\n",
      "+---------------------------------+-----------------------------------------+------------------------------------------+\n",
      "|Fazenda Sorriso MetÃ¡lico Airport|[-48.21527862548828, -17.456666946411133]|{-0.8415153617812164, -0.3046763146389388}|\n",
      "|Lilienthal Airport               |[-88.09819793701172, 45.932701110839844] |{-1.5376036190745306, 0.8016768687186121} |\n",
      "|Wabasis Lake Airport             |[-85.39920043945312, 43.12839889526367]  |{-1.4904972262390455, 0.7527325618358361} |\n",
      "+---------------------------------+-----------------------------------------+------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_rads.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**expr**: превращает SQL-выражение в Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------------------------------------+------------------------------------------+\n",
      "|name                             |coords                                   |coords_rad                                |\n",
      "+---------------------------------+-----------------------------------------+------------------------------------------+\n",
      "|Fazenda Sorriso MetÃ¡lico Airport|[-48.21527862548828, -17.456666946411133]|{-0.8415153617812164, -0.3046763146389388}|\n",
      "|Lilienthal Airport               |[-88.09819793701172, 45.932701110839844] |{-1.5376036190745306, 0.8016768687186121} |\n",
      "|Wabasis Lake Airport             |[-85.39920043945312, 43.12839889526367]  |{-1.4904972262390455, 0.7527325618358361} |\n",
      "+---------------------------------+-----------------------------------------+------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_coords\\\n",
    ".withColumn(\"coords_rad\", F.expr(\"struct(radians(coords[0]) as lat, radians(coords[1]) as lon)\"))\\\n",
    ".show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'struct(radians(coords[0]) AS lat, radians(coords[1]) AS lon)'>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.expr(\"struct(radians(coords[0]) as lat, radians(coords[1]) as lon)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'struct(RADIANS(coords[0]) AS lat, RADIANS(coords[1]) AS lon)'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.struct(\n",
    "    F.radians(F.col(\"coords\")[0]).alias(\"lat\"),\n",
    "    F.radians(F.col(\"coords\")[1]).alias(\"lon\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые функции есть в Java/Scala, но нет в Python.<br/> \n",
    "Т.к. под капотом Spark всё равно работает на Scala, их можно вызывать, обернув в **java_method**. Это доступно только внутри *expr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                name|              coords|                uuid|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Fazenda Sorriso M...|[-48.215278625488...|aee94a9b-fe74-40d...|\n",
      "|  Lilienthal Airport|[-88.098197937011...|19358847-f66a-494...|\n",
      "|Wabasis Lake Airport|[-85.399200439453...|e6dd3928-223b-49c...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_coords\\\n",
    ".withColumn(\"uuid\", F.expr(\"java_method('java.util.UUID', 'randomUUID')\"))\\\n",
    ".show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нужной функции нет среди встроенных или среди Java-методов, можно написать свою **UDF** – User Defined Function. <br/>\n",
    "**У этого решения есть минусы**:\n",
    "- Встроенные функции оптимизируются, пользовательские непрозрачны для оптимизатора\n",
    "- В Python UDF будут передавать данные между JVM и Python, снижая производительность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "def rads(coords: list) -> dict:\n",
    "    return {\n",
    "        \"lat\": float(coords[0]) * math.pi / 180,\n",
    "        \"lon\": float(coords[1]) * math.pi / 180\n",
    "    }\n",
    "\n",
    "output_type = StructType([\n",
    "    StructField(\"lat\", DoubleType()),\n",
    "    StructField(\"lon\", DoubleType())\n",
    "])\n",
    "\n",
    "rads_udf = F.udf(lambda c: rads(c), output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------------------------------------+------------------------------------------+\n",
      "|name                             |coords                                   |coords_rad                                |\n",
      "+---------------------------------+-----------------------------------------+------------------------------------------+\n",
      "|Fazenda Sorriso MetÃ¡lico Airport|[-48.21527862548828, -17.456666946411133]|{-0.8415153617812164, -0.3046763146389388}|\n",
      "|Lilienthal Airport               |[-88.09819793701172, 45.932701110839844] |{-1.5376036190745304, 0.8016768687186121} |\n",
      "|Wabasis Lake Airport             |[-85.39920043945312, 43.12839889526367]  |{-1.4904972262390455, 0.7527325618358361} |\n",
      "+---------------------------------+-----------------------------------------+------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_coords\\\n",
    ".withColumn(\"coords_rad\", rads_udf(name_coords.coords))\\\n",
    ".show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Группировки, агрегации, оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод [groupBy](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) порождает промежуточный DSL-объект [GroupedData](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#pyspark.sql.GroupedData), на котором вызывается трансформация agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f1e5cce4d30>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.groupBy(\"iso_country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В agg можно использовать агрегирующие функции из [pyspark.sql.functions](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html?highlight=aggregate#module-pyspark.sql.functions) (помечены как Aggregate function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|iso_country|count|\n",
      "+-----------+-----+\n",
      "|         DZ|   61|\n",
      "|         LT|   59|\n",
      "|         MM|   75|\n",
      "|         CI|   26|\n",
      "|         TC|    8|\n",
      "|         FI|  112|\n",
      "|         PM|    2|\n",
      "|         AZ|   35|\n",
      "|         SC|   16|\n",
      "|         UA|  195|\n",
      "|         KI|   22|\n",
      "|         RO|   60|\n",
      "|         ZM|  103|\n",
      "|         SL|   12|\n",
      "|         NL|  115|\n",
      "|         SB|   38|\n",
      "|         LA|   20|\n",
      "|         BS|   65|\n",
      "|         BW|  129|\n",
      "|         MN|   30|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "codes.groupBy(\"iso_country\").agg(F.count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------+\n",
      "|substring(iso_country, 0, 1)|max(elevation_ft)|\n",
      "+----------------------------+-----------------+\n",
      "|                           K|            10200|\n",
      "|                           F|            11647|\n",
      "|                           Q|              130|\n",
      "|                           E|             9649|\n",
      "|                           T|            11962|\n",
      "|                           B|            14360|\n",
      "|                           Y|             7216|\n",
      "|                           M|            10074|\n",
      "|                           L|            10400|\n",
      "|                           U|            29977|\n",
      "|                           V|             5269|\n",
      "|                           D|             4518|\n",
      "|                           O|             6500|\n",
      "|                           C|            14472|\n",
      "|                           J|             2940|\n",
      "|                           Z|             6464|\n",
      "|                           A|            13000|\n",
      "|                           N|            12400|\n",
      "|                           X|             2168|\n",
      "|                           W|              131|\n",
      "+----------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "codes.groupBy(F.substring(\"iso_country\", 0, 1)).agg(F.max(\"elevation_ft\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оконные функции выполняют вычисления над окнами данных без группировки. <br/>\n",
    "Они используются так же, как встроенные функции. Окно определяется через класс [Window](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#pyspark.sql.Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"iso_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+-------------+\n",
      "|                name|iso_country|iso_region|max_elevation|\n",
      "+--------------------+-----------+----------+-------------+\n",
      "|Andorra la Vella ...|         AD|     AD-07|         3450|\n",
      "|      CamÃ­ Heliport|         AD|     AD-04|         3450|\n",
      "|Al Hamra Aux Airport|         AE|     AE-AZ|          869|\n",
      "|Sharjah Internati...|         AE|     AE-SH|          869|\n",
      "|Al Mushrif Palace...|         AE|     AE-AZ|          869|\n",
      "|Al Maktoum Intern...|         AE|     AE-DU|          869|\n",
      "| Schumacher Heliport|         AE|     AE-DU|          869|\n",
      "|     Arzanah Airport|         AE|     AE-AZ|          869|\n",
      "|Sir Bani Yas Airport|         AE|     AE-AZ|          869|\n",
      "|       KIZAD Airport|         AE|     AE-AZ|          869|\n",
      "|     Dubai Creek SPB|         AE|     AE-DU|          869|\n",
      "|       Al Saqr Field|         AE|     AE-RK|          869|\n",
      "|       Umm Al Quwain|         AE|     AE-UQ|          869|\n",
      "|Al Ghuwaifat Cust...|         AE|     AE-DU|          869|\n",
      "|Dubai Festival Ci...|         AE|     AE-DU|          869|\n",
      "|  Al Dhafra Air Base|         AE|     AE-AZ|          869|\n",
      "|Ghantoot Hotel 3 ...|         AE|     AE-AZ|          869|\n",
      "|Jebel Ali Seaplan...|         AE|     AE-DU|          869|\n",
      "|       Delma Airport|         AE|     AE-AZ|          869|\n",
      "|       Asab Airfield|         AE|     AE-AZ|          869|\n",
      "+--------------------+-----------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "codes\\\n",
    ".withColumn(\"max_elevation\", F.max(\"elevation_ft\").over(w))\\\n",
    ".select(\"name\", \"iso_country\", \"iso_region\", \"max_elevation\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [name#479, iso_country#482, iso_region#483, max_elevation#1262]\n",
      "   +- Window [max(elevation_ft#480) windowspecdefinition(iso_country#482, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS max_elevation#1262], [iso_country#482]\n",
      "      +- Sort [iso_country#482 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(iso_country#482, 200), ENSURE_REQUIREMENTS, [plan_id=1568]\n",
      "            +- FileScan parquet [name#479,elevation_ft#480,iso_country#482,iso_region#483] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<name:string,elevation_ft:int,iso_country:string,iso_region:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "codes\\\n",
    ".withColumn(\"max_elevation\", F.max(\"elevation_ft\").over(w))\\\n",
    ".select(\"name\", \"iso_country\", \"iso_region\", \"max_elevation\")\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединение реализуется через метод [union](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union).\n",
    "* Не выполняет дедупликацию – одинаковые строки будут дублироваться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|str|int|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  a|  1|\n",
      "|  c|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ab = spark.createDataFrame([\n",
    "    [\"a\", 1],\n",
    "    [\"b\", 2]\n",
    "], schema=StructType([\n",
    "    StructField(\"str\", StringType()),\n",
    "    StructField(\"int\", IntegerType())    \n",
    "]))\n",
    "\n",
    "ac = spark.createDataFrame([\n",
    "    [\"a\", 1],\n",
    "    [\"c\", 3]\n",
    "], schema=StructType([\n",
    "    StructField(\"str\", StringType()),\n",
    "    StructField(\"int\", IntegerType())    \n",
    "]))\n",
    "\n",
    "ab.union(ac).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Резолвит колонки **по порядку**, а не по имени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|str|int|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  1|  a|\n",
      "|  3|  c|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ab1 = spark.createDataFrame([\n",
    "    [\"a\", 1],\n",
    "    [\"b\", 2]\n",
    "], schema=StructType([\n",
    "    StructField(\"str\", StringType()),\n",
    "    StructField(\"int\", IntegerType())    \n",
    "]))\n",
    "\n",
    "ac1 = spark.createDataFrame([\n",
    "    [1, \"a\"],\n",
    "    [3, \"c\"]\n",
    "    \n",
    "# порядок колонок в DataFrame недетерминирован, так что по сути ac1 эквивалентен ac...    \n",
    "], schema=StructType([\n",
    "    StructField(\"int\", IntegerType()),\n",
    "    StructField(\"str\", StringType())   \n",
    "]))\n",
    "\n",
    "#... но не для union\n",
    "ab1.union(ac1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- str: string (nullable = true)\n",
      " |-- int: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ab1.union(ac1).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Чтобы правильно резолвить колонки, лучше всегда использовать [unionByName](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.unionByName):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|str|int|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  a|  1|\n",
      "|  c|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ab1.unionByName(ac1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединение реализуется через трансформацию [join](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join).\n",
    "Обычные типы джойна:\n",
    "- `'inner'`\n",
    "- `'cross'`\n",
    "- `'left'` == `'left_outer'`\n",
    "- `'right'` == `'right_outer'`\n",
    "- `'full'` == `'outer'` == `'full_outer'`\n",
    "\n",
    "<img src='https://i.stack.imgur.com/vPPHT.png' width=50% height=auto align=left></img><br/><br/>\n",
    "Картинка из статьи: http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас есть данные по количеству населения в разных странах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".csv(\"population_by_country_2020.csv\")\\\n",
    ".withColumnRenamed(\"Country (or dependency)\", \"country\")\\\n",
    ".withColumnRenamed(\"Population (2020)\", \"population\")\\\n",
    ".select(\"country\", \"population\")\\\n",
    ".repartition(2)\\\n",
    ".write.mode(\"overwrite\").parquet(\"/tmp/data/population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             country|population|\n",
      "+--------------------+----------+\n",
      "|             Albania|   2878420|\n",
      "|       New Caledonia|    284938|\n",
      "|            Tanzania|  59368313|\n",
      "|        South Africa|  59154802|\n",
      "|              Sweden|  10086531|\n",
      "|             Tokelau|      1354|\n",
      "|              France|  65244628|\n",
      "|            Ethiopia| 114357494|\n",
      "| Trinidad and Tobago|   1398579|\n",
      "|            Paraguay|   7114524|\n",
      "|            Kiribati|    119069|\n",
      "|           Argentina|  45111229|\n",
      "|      Faeroe Islands|     48826|\n",
      "|           Venezuela|  28451828|\n",
      "|            Portugal|  10202571|\n",
      "|              Guyana|    785788|\n",
      "|          Costa Rica|   5084636|\n",
      "|           Australia|  25439164|\n",
      "|            Botswana|   2341649|\n",
      "|Central African R...|   4812256|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "population = spark.read.parquet(\"/tmp/data/population\")\n",
    "population.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем сопоставить население аэропортам через код страны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".csv(\"wikipedia-iso-country-codes.csv\")\\\n",
    ".withColumnRenamed(\"English short name lower case\", \"name\")\\\n",
    ".withColumnRenamed(\"Alpha-2 code\", \"code_2\")\\\n",
    ".withColumnRenamed(\"Alpha-3 code\", \"code_3\")\\\n",
    ".select(\"name\", \"code_2\", \"code_3\")\\\n",
    ".repartition(2)\\\n",
    ".write.mode(\"overwrite\").parquet(\"/tmp/data/country_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------+\n",
      "|            name|code_2|code_3|\n",
      "+----------------+------+------+\n",
      "|        Mongolia|    MN|   MNG|\n",
      "|          Tuvalu|    TV|   TUV|\n",
      "|     South Korea|    KR|   KOR|\n",
      "|            Oman|    OM|   OMN|\n",
      "|         Ireland|    IE|   IRL|\n",
      "|          Russia|    RU|   RUS|\n",
      "|      Kyrgyzstan|    KG|   KGZ|\n",
      "|       Venezuela|    VE|   VEN|\n",
      "|           Ghana|    GH|   GHA|\n",
      "|    Burkina Faso|    BF|   BFA|\n",
      "|          Rwanda|    RW|   RWA|\n",
      "|          Guinea|    GN|   GIN|\n",
      "|     Netherlands|    NL|   NLD|\n",
      "|           Qatar|    QA|   QAT|\n",
      "|      Guadeloupe|    GP|   GLP|\n",
      "|         Andorra|    AD|   AND|\n",
      "|        Cambodia|    KH|   KHM|\n",
      "|Papua New Guinea|    PG|   PNG|\n",
      "|        Suriname|    SR|   SUR|\n",
      "|         Nigeria|    NG|   NGA|\n",
      "+----------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_codes = spark.read.parquet(\"/tmp/data/country_codes\")\n",
    "country_codes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = codes.join(country_codes, F.col(\"iso_country\") == F.col(\"code_2\")) # по умолчанию inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+------------+---------+-----------+----------+--------------------+--------+---------+----------+--------------------+-------------+------+------+\n",
      "|  ident|          type|                name|elevation_ft|continent|iso_country|iso_region|        municipality|gps_code|iata_code|local_code|         coordinates|         name|code_2|code_3|\n",
      "+-------+--------------+--------------------+------------+---------+-----------+----------+--------------------+--------+---------+----------+--------------------+-------------+------+------+\n",
      "|   SIYK| small_airport|Fazenda Sorriso M...|        2605|       SA|         BR|     BR-GO|              Urutai|    SIYK|     null|      null|-48.2152786254882...|       Brazil|    BR|   BRA|\n",
      "|   MI92| small_airport|  Lilienthal Airport|        1250|       NA|         US|     US-MI|       Iron Mountain|    MI92|     null|      MI92|-88.0981979370117...|United States|    US|   USA|\n",
      "|   MI80| small_airport|Wabasis Lake Airport|         892|       NA|         US|     US-MI|          Greenville|    MI80|     null|      MI80|-85.3992004394531...|United States|    US|   USA|\n",
      "|   SNXK| small_airport|Fazenda Lagoa do ...|         558|       SA|         BR|     BR-PA|SÃ£o FÃ©lix Do Xingu|    SNXK|     null|      null|-52.7691688537597...|       Brazil|    BR|   BRA|\n",
      "|FR-0042|      heliport|Centre Hospitalie...|         781|       EU|         FR|    FR-BFC|               Dijon|    null|     null|      null|5.026944160461426...|       France|    FR|   FRA|\n",
      "|   SCPU| small_airport|      Peulla Airport|         900|       SA|         CL|     CL-LL|              Peulla|    SCPU|     null|      null|-72.0083312988281...|        Chile|    CL|   CHL|\n",
      "|   LZZI|medium_airport|     Å½ilina Airport|        1020|       EU|         SK|     SK-ZI|             Å½ilina|    LZZI|      ILZ|      null|18.6135005951, 49...|     Slovakia|    SK|   SVK|\n",
      "|   OR14| small_airport|     Juntura Airport|        3034|       NA|         US|     US-OR|             Juntura|    OR14|     null|      OR14|-118.064002990722...|United States|    US|   USA|\n",
      "|   SIVA|      heliport|Cereal Citrus Hel...|        3183|       SA|         BR|     BR-DF|           BrasÃ­lia|    SIVA|     null|      null|-47.3694458007999...|       Brazil|    BR|   BRA|\n",
      "|   CT84|      heliport|Partyka Chevrolet...|          50|       NA|         US|     US-CT|              Hamden|    CT84|     null|      CT84|-72.9086990356445...|United States|    US|   USA|\n",
      "|MX-0979|      heliport|Plaza Insurgentes...|        7782|       NA|         MX|    MX-DIF|      Alvaro Obregon|    null|     null|       HPF|-99.187649, 19.35...|       Mexico|    MX|   MEX|\n",
      "|SK-0058| small_airport|RimavskÃ¡ SeÄ Ai...|        null|       EU|         SK|    SK-U-A|                null|    null|     null|      null|20.2502328, 48.31...|     Slovakia|    SK|   SVK|\n",
      "|   0OA2|      heliport|Cleveland Clinic,...|         983|       NA|         US|     US-OH|              Medina|    0OA2|     null|      0OA2|-81.836745, 41.13...|United States|    US|   USA|\n",
      "|    SNQ| small_airport|San QuintÃ­n Mili...|          68|       NA|         MX|    MX-BCN|Military Camp Num...|    null|      SNQ|      null|  -115.9462, 30.5288|       Mexico|    MX|   MEX|\n",
      "|KR-0127|      heliport|Geomsan-dong Helipad|        null|       AS|         KR|     KR-41|       Geomsan-ddong|    null|     null|      null|126.751521, 37.79612|  South Korea|    KR|   KOR|\n",
      "|   CMF4| small_airport|Port Hope (Millso...|         525|       NA|         CA|     CA-ON|                null|    CMF4|     null|      CMF4|-78.428691, 43.98...|       Canada|    CA|   CAN|\n",
      "|MX-1216| small_airport|MontaÃ±as del Gua...|          23|       NA|         MX|    MX-TAM|      Villa Gonzalez|    null|     null|       MDG|-98.340972, 22.55...|       Mexico|    MX|   MEX|\n",
      "|   KBED|medium_airport|Laurence G Hansco...|         133|       NA|         US|     US-MA|             Bedford|    KBED|      BED|       BED|-71.28900146, 42....|United States|    US|   USA|\n",
      "|   LOIJ| small_airport|St. Johann In Tir...|        2198|       EU|         AT|      AT-7| St. Johann In Tirol|    LOIJ|     null|      null|    12.4497, 47.5201|      Austria|    AT|   AUT|\n",
      "|   81MI|      heliport|Three Rivers Heal...|         818|       NA|         US|     US-MI|        Three Rivers|    81MI|     null|      81MI|-85.648708, 41.93...|United States|    US|   USA|\n",
      "+-------+--------------+--------------------+------------+---------+-----------+----------+--------------------+--------+---------+----------+--------------------+-------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Почему Spark не посчитал одинаковые имена колонок ошибкой? <br/>\n",
    "Потому что он хранит всю цепочку преобразований датасетов и внутри всё ещё видит, что это колонки от разных DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [iso_country#482], [code_2#1476], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(iso_country#482)\n",
      "   :  +- FileScan parquet [ident#477,type#478,name#479,elevation_ft#480,continent#481,iso_country#482,iso_region#483,municipality#484,gps_code#485,iata_code#486,local_code#487,coordinates#488] Batched: true, DataFilters: [isnotnull(iso_country#482)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=1907]\n",
      "      +- Filter isnotnull(code_2#1476)\n",
      "         +- FileScan parquet [name#1475,code_2#1476,code_3#1477] Batched: true, DataFilters: [isnotnull(code_2#1476)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/country_codes], PartitionFilters: [], PushedFilters: [IsNotNull(code_2)], ReadSchema: struct<name:string,code_2:string,code_3:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'name' is ambiguous, could be: name, name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mairports\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1685\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \n\u001b[1;32m   1667\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1685\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Reference 'name' is ambiguous, could be: name, name."
     ]
    }
   ],
   "source": [
    "airports.select(\"name\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|Fazenda Sorriso M...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.select(codes.name).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформация **withColumnRenamed** переименовывает колонки без необходимости делать `withColumn(newName, col(oldName))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = codes\\\n",
    ".withColumnRenamed(\"name\", \"airport_name\")\\\n",
    ".join(country_codes.withColumnRenamed(\"name\", \"country\"),\n",
    "     F.col(\"iso_country\") == F.col(\"code_2\"),\n",
    "     \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+------+\n",
      "|        airport_name|      country|iso_country|code_2|\n",
      "+--------------------+-------------+-----------+------+\n",
      "|Fazenda Sorriso M...|       Brazil|         BR|    BR|\n",
      "|  Lilienthal Airport|United States|         US|    US|\n",
      "|Wabasis Lake Airport|United States|         US|    US|\n",
      "+--------------------+-------------+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.select(\"airport_name\", \"country\", \"iso_country\", \"code_2\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+------+------+----------+\n",
      "|      country|ident|         type|        airport_name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|code_2|code_3|population|\n",
      "+-------------+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+------+------+----------+\n",
      "|       Brazil| SIYK|small_airport|Fazenda Sorriso M...|        2605|       SA|         BR|     BR-GO|       Urutai|    SIYK|     null|      null|-48.2152786254882...|    BR|   BRA| 212253150|\n",
      "|United States| MI92|small_airport|  Lilienthal Airport|        1250|       NA|         US|     US-MI|Iron Mountain|    MI92|     null|      MI92|-88.0981979370117...|    US|   USA| 330610570|\n",
      "|United States| MI80|small_airport|Wabasis Lake Airport|         892|       NA|         US|     US-MI|   Greenville|    MI80|     null|      MI80|-85.3992004394531...|    US|   USA| 330610570|\n",
      "+-------------+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_pop = airports.join(population, on=[\"country\"], how=\"left\")\n",
    "\n",
    "airports_pop.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Сколько в крупных странах маленьких аэропортов?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = airports_pop\\\n",
    ".where(\"population > 100000000\")\\\n",
    ".groupBy(\"country\")\\\n",
    ".agg(F.count(F.when(F.col(\"type\") == 'small_airport', F.lit(1))).alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      country|count|\n",
      "+-------------+-----+\n",
      "|       Russia|  554|\n",
      "|  Philippines|  102|\n",
      "|United States|13559|\n",
      "|        India|  158|\n",
      "|        China|   97|\n",
      "|      Nigeria|   19|\n",
      "|   Bangladesh|    6|\n",
      "|       Mexico|  964|\n",
      "|    Indonesia|  399|\n",
      "|     Ethiopia|   43|\n",
      "|       Brazil| 3489|\n",
      "|        Japan|   54|\n",
      "|        Egypt|   41|\n",
      "|     Pakistan|   80|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[country#1604], functions=[count(CASE WHEN (type#478 = small_airport) THEN 1 END)])\n",
      "   +- Exchange hashpartitioning(country#1604, 200), ENSURE_REQUIREMENTS, [plan_id=2557]\n",
      "      +- HashAggregate(keys=[country#1604], functions=[partial_count(CASE WHEN (type#478 = small_airport) THEN 1 END)])\n",
      "         +- Project [country#1604, type#478]\n",
      "            +- BroadcastHashJoin [country#1604], [country#1409], Inner, BuildRight, false\n",
      "               :- Project [type#478, country#1604]\n",
      "               :  +- BroadcastHashJoin [iso_country#482], [code_2#1476], Inner, BuildRight, false\n",
      "               :     :- FileScan parquet [type#478,iso_country#482] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<type:string,iso_country:string>\n",
      "               :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=2548]\n",
      "               :        +- Project [name#1475 AS country#1604, code_2#1476]\n",
      "               :           +- Filter (isnotnull(code_2#1476) AND isnotnull(name#1475))\n",
      "               :              +- FileScan parquet [name#1475,code_2#1476] Batched: true, DataFilters: [isnotnull(code_2#1476), isnotnull(name#1475)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/country_codes], PartitionFilters: [], PushedFilters: [IsNotNull(code_2), IsNotNull(name)], ReadSchema: struct<name:string,code_2:string>\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=2552]\n",
      "                  +- Project [country#1409]\n",
      "                     +- Filter ((isnotnull(population#1410) AND (population#1410 > 100000000)) AND isnotnull(country#1409))\n",
      "                        +- FileScan parquet [country#1409,population#1410] Batched: true, DataFilters: [isnotnull(population#1410), (population#1410 > 100000000), isnotnull(country#1409)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/population], PartitionFilters: [], PushedFilters: [IsNotNull(population), GreaterThan(population,100000000), IsNotNull(country)], ReadSchema: struct<country:string,population:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Spark \"проталкивает\" предикаты через джойны ближе к чтению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Сколько есть маленьких стран, где *нет* аэропортов?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные, \"ненастоящие\" типы джойна – фильтрации одной таблицы по другой:\n",
    "- `'left_semi'`: оставляет записи из левой таблицы, для которых *есть* соответствие в правой\n",
    "- `'left_anti'`: оставляет записи из левой таблицы, для которых *нет* соответствия в правой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans2 = population\\\n",
    ".where(\"population <= 100000000\")\\\n",
    ".join(airports, \"country\", \"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             country|population|\n",
      "+--------------------+----------+\n",
      "|  State of Palestine|   5076280|\n",
      "|             Tokelau|      1354|\n",
      "|Czech Republic (C...|  10705012|\n",
      "+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+------------+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+------+------+----------+\n",
      "|country|ident|type|airport_name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|code_2|code_3|population|\n",
      "+-------+-----+----+------------+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+------+------+----------+\n",
      "+-------+-----+----+------------+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_pop.where(\"country = 'Cabo Verde'\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [country#1409], [country#1604], LeftAnti\n",
      "   :- Sort [country#1409 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(country#1409, 200), ENSURE_REQUIREMENTS, [plan_id=3291]\n",
      "   :     +- Filter (isnotnull(population#1410) AND (population#1410 <= 100000000))\n",
      "   :        +- FileScan parquet [country#1409,population#1410] Batched: true, DataFilters: [isnotnull(population#1410), (population#1410 <= 100000000)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/population], PartitionFilters: [], PushedFilters: [IsNotNull(population), LessThanOrEqual(population,100000000)], ReadSchema: struct<country:string,population:int>\n",
      "   +- Sort [country#1604 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(country#1604, 200), ENSURE_REQUIREMENTS, [plan_id=3292]\n",
      "         +- Project [country#1604]\n",
      "            +- BroadcastHashJoin [iso_country#482], [code_2#1476], Inner, BuildRight, false\n",
      "               :- FileScan parquet [iso_country#482] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<iso_country:string>\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=3286]\n",
      "                  +- Project [name#1475 AS country#1604, code_2#1476]\n",
      "                     +- Filter (isnotnull(code_2#1476) AND isnotnull(name#1475))\n",
      "                        +- FileScan parquet [name#1475,code_2#1476] Batched: true, DataFilters: [isnotnull(code_2#1476), isnotnull(name#1475)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/country_codes], PartitionFilters: [], PushedFilters: [IsNotNull(code_2), IsNotNull(name)], ReadSchema: struct<name:string,code_2:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans2.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^ Мы заново пересчитываем JOIN codes и country_codes. Почему?**<br/><br/>\n",
    "Ответ: join – трансформация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кэширование и контрольные точки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод [persist(storageLevel)](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.persist) позволяет сохранить данные в результате цепочки преобразований и не пересчитывать заново.<br/>\n",
    "Сохранять можно в память *экзекьюторов* и/или на диск, с репликацией или без, и даже в off-heap память: см. [StorageLevel](https://spark.apache.org/docs/2.3.2/api/python/pyspark.html?highlight=storagelevel#pyspark.StorageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ident: string, type: string, airport_name: string, elevation_ft: int, continent: string, iso_country: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string, country: string, code_2: string, code_3: string]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# По умолчанию MEMORY_AND_DISK: сохраняем в память, при заполнении памяти сбрасываем на диск, без репликации\n",
    "\n",
    "ap_persisted = airports.persist() \n",
    "ap_persisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/12 11:39:28 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ident: string, type: string, airport_name: string, elevation_ft: int, continent: string, iso_country: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string, country: string, code_2: string, code_3: string]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.cache() # эквивалентно persist без параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/12 11:39:31 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ident: string, type: string, airport_name: string, elevation_ft: int, continent: string, iso_country: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string, country: string, code_2: string, code_3: string]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "airports.persist(storageLevel=StorageLevel.DISK_ONLY_2) # сразу сохранять на диск с фактором репликации 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist – это *трансформация*: она помечает узел в плане так, что при следующем *действии* он будет закэширован для всех потомков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 158:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|        airport_name|elevation_ft|\n",
      "+--------------------+------------+\n",
      "|Fazenda Sorriso M...|        2605|\n",
      "|  Lilienthal Airport|        1250|\n",
      "|Wabasis Lake Airport|         892|\n",
      "+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ap_persisted.select(\"airport_name\", \"elevation_ft\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [airport_name#1591, elevation_ft#480]\n",
      "   +- InMemoryRelation [ident#477, type#478, airport_name#1591, elevation_ft#480, continent#481, iso_country#482, iso_region#483, municipality#484, gps_code#485, iata_code#486, local_code#487, coordinates#488, country#1604, code_2#1476, code_3#1477], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(2) BroadcastHashJoin [iso_country#482], [code_2#1476], LeftOuter, BuildRight, false\n",
      "            :- *(2) Project [ident#477, type#478, name#479 AS airport_name#1591, elevation_ft#480, continent#481, iso_country#482, iso_region#483, municipality#484, gps_code#485, iata_code#486, local_code#487, coordinates#488]\n",
      "            :  +- *(2) ColumnarToRow\n",
      "            :     +- FileScan parquet [ident#477,type#478,name#479,elevation_ft#480,continent#481,iso_country#482,iso_region#483,municipality#484,gps_code#485,iata_code#486,local_code#487,coordinates#488] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=3330]\n",
      "               +- *(1) Project [name#1475 AS country#1604, code_2#1476, code_3#1477]\n",
      "                  +- *(1) Filter isnotnull(code_2#1476)\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet [name#1475,code_2#1476,code_3#1477] Batched: true, DataFilters: [isnotnull(code_2#1476)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/country_codes], PartitionFilters: [], PushedFilters: [IsNotNull(code_2)], ReadSchema: struct<name:string,code_2:string,code_3:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap_persisted.select(\"airport_name\", \"elevation_ft\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Spark сохраняет план вычисления вместе с данными: если они будут вытеснены, их можно будет пересчитать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если план слишком большой, он будет долго обрабатываться и может даже не поместиться в память.<br/>\n",
    "Метод [checkpoint](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.checkpoint) сбрасывает посчитанные данные **на HDFS** (или в локальную файловую систему для standalone mode) и **отбрасывает план**.<br/>\n",
    "Checkpoint – это *действие*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"/tmp/checkpoints\") # обязательно нужно указать в контексте директорию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ap_checkpoint = airports.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m0be31925-df12-46c4-b121-208dd1759170\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /tmp/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/tmp/checkpoints/2de858f1-b4b8-4b7e-a89f-62120d53fd34': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls /tmp/checkpoints/2de858f1-b4b8-4b7e-a89f-62120d53fd34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|        airport_name|elevation_ft|\n",
      "+--------------------+------------+\n",
      "|Fazenda Sorriso M...|        2605|\n",
      "|  Lilienthal Airport|        1250|\n",
      "|Wabasis Lake Airport|         892|\n",
      "+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap_checkpoint.select(\"airport_name\", \"elevation_ft\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [airport_name#1591, elevation_ft#480]\n",
      "+- *(1) Scan ExistingRDD[ident#477,type#478,airport_name#1591,elevation_ft#480,continent#481,iso_country#482,iso_region#483,municipality#484,gps_code#485,iata_code#486,local_code#487,coordinates#488,country#1604,code_2#1476,code_3#1477]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap_checkpoint.select(\"airport_name\", \"elevation_ft\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ident: string, type: string, airport_name: string, elevation_ft: int, continent: string, iso_country: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string, country: string, code_2: string, code_3: string]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Убирает DF из кэша\n",
    "ap_persisted.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Без материализации | Persist  | Checkpoint  | \n",
    "|---|---|---|---|\n",
    "| **Пересчитывается**  | Каждый раз  | Если материализация потеряна  | Не пересчитывается  |\n",
    "| **Промежуточные данные**  |  Не сохраняются |  Сохраняются в памяти/на диске | Сохраняются в файловой системе  |\n",
    "| **Оптимизации**  | Меняют план как угодно  | Не меняют план до материализации  | Не меняют план до материализации  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle и Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос: как сджойнить два файла с разных узлов? Как сопоставить нужные ключи?**<br/>\n",
    "**Ответы:**\n",
    "1. хэшируем ключи, прикапываем где-то (на отдельной ноде), потом сопоставляем\n",
    "2. партиционировать сразу по одному ключу (bucketing)\n",
    "3. залить один файл к другому\n",
    "4. прочитать ключи, обменяться частями\n",
    "5. перемешать между нодами ключи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffle** – операция разбиения данных на каждом экзекьюторе на N партиций по (чаще всего) хэшу от ключа (*Shuffle Write*) и обмена этими партициями между экзекьюторами (*Shuffle Read*). По умолчанию он нужен тогда, когда результат зависит от данных на других экзекьюторах (*широкие трансформации*). <br/>\n",
    "Широкие/узкие трансформации: https://sauravomar01.medium.com/wide-vs-narrow-dependencies-in-apache-spark-2cd33bf7ed7d <br/>\n",
    "Погружение в shuffle: https://medium.com/@philipp.brunenberg/understanding-apache-spark-shuffle-85644d90c8c6 <br/><br/>\n",
    "Самое главное: **shuffle – это долго и затратно**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\") # то самое N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос: какая ещё трансформация в этом уроке порождает шаффл?**<br/>\n",
    "**Ответы:** groupBy, orderBy, оконные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [type#478 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(type#478 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=3412]\n",
      "      +- Project [type#478]\n",
      "         +- BroadcastHashJoin [iso_country#482], [code_2#1476], LeftOuter, BuildRight, false\n",
      "            :- FileScan parquet [type#478,iso_country#482] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<type:string,iso_country:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=3408]\n",
      "               +- Filter isnotnull(code_2#1476)\n",
      "                  +- FileScan parquet [code_2#1476] Batched: true, DataFilters: [isnotnull(code_2#1476)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/country_codes], PartitionFilters: [], PushedFilters: [IsNotNull(code_2)], ReadSchema: struct<code_2:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.sort(\"type\").select(\"type\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [type#478 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(type#478 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=3445]\n",
      "      +- Project [type#478]\n",
      "         +- BroadcastHashJoin [iso_country#482], [code_2#1476], LeftOuter, BuildRight, false\n",
      "            :- FileScan parquet [type#478,iso_country#482] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<type:string,iso_country:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=3441]\n",
      "               +- Filter isnotnull(code_2#1476)\n",
      "                  +- FileScan parquet [code_2#1476] Batched: true, DataFilters: [isnotnull(code_2#1476)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/country_codes], PartitionFilters: [], PushedFilters: [IsNotNull(code_2)], ReadSchema: struct<code_2:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.orderBy(\"type\").select(\"type\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[iso_country#482], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(iso_country#482, 200), ENSURE_REQUIREMENTS, [plan_id=3458]\n",
      "      +- HashAggregate(keys=[iso_country#482], functions=[partial_count(1)])\n",
      "         +- FileScan parquet [iso_country#482] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<iso_country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# здесь count – это эквивалент agg(F.count(\"*\"))\n",
    "codes.groupBy(\"iso_country\").count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|iso_country|count|\n",
      "+-----------+-----+\n",
      "|         DZ|   61|\n",
      "|         LT|   59|\n",
      "|         MM|   75|\n",
      "+-----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "codes.groupBy(\"iso_country\").count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[iso_country: string, count: bigint]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.groupBy(\"iso_country\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.groupBy(\"iso_country\").count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broadcast** – операция рассылания данных на все экзекьюторы. Эти данные будут сохранены в памяти каждого процесса для быстрого доступа. <br/>\n",
    "В случае с JOIN можно бродкастить *датафрейм* – если он достаточно мал, чтобы поместиться в память экзекьютора (и ещё на обработку осталось)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760b'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\") # это верхняя граница размера DF в байтах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numfmt: invalid suffix in input: ‘10485760b’\n"
     ]
    }
   ],
   "source": [
    "!echo 26214400 | numfmt --to=iec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = codes.join(country_codes, F.col(\"iso_country\") == F.col(\"code_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [iso_country#482], [code_2#1476], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(iso_country#482)\n",
      "   :  +- FileScan parquet [ident#477,type#478,name#479,elevation_ft#480,continent#481,iso_country#482,iso_region#483,municipality#484,gps_code#485,iata_code#486,local_code#487,coordinates#488] Batched: true, DataFilters: [isnotnull(iso_country#482)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=3621]\n",
      "      +- Filter isnotnull(code_2#1476)\n",
      "         +- FileScan parquet [name#1475,code_2#1476,code_3#1477] Batched: true, DataFilters: [isnotnull(code_2#1476)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/country_codes], PartitionFilters: [], PushedFilters: [IsNotNull(code_2)], ReadSchema: struct<name:string,code_2:string,code_3:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+-------------+------+------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|         name|code_2|code_3|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+-------------+------+------+\n",
      "| SIYK|small_airport|Fazenda Sorriso M...|        2605|       SA|         BR|     BR-GO|       Urutai|    SIYK|     null|      null|-48.2152786254882...|       Brazil|    BR|   BRA|\n",
      "| MI92|small_airport|  Lilienthal Airport|        1250|       NA|         US|     US-MI|Iron Mountain|    MI92|     null|      MI92|-88.0981979370117...|United States|    US|   USA|\n",
      "| MI80|small_airport|Wabasis Lake Airport|         892|       NA|         US|     US-MI|   Greenville|    MI80|     null|      MI80|-85.3992004394531...|United States|    US|   USA|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+-------------+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") # выключает автоматический Broadcast Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.31:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Apache Spark Lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1e5c437d60>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Перезапускаем сессию для применения\n",
    "spark.stop()\n",
    "spark = SparkSession.builder\\\n",
    ".appName(\"Apache Spark Lecture\")\\\n",
    ".config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# С новой сессией все датафреймы нужно создавать заново\n",
    "codes = spark.read.parquet(\"/tmp/data/airport_codes\")\n",
    "country_codes = spark.read.parquet(\"/tmp/data/country_codes\")\n",
    "join = codes.join(country_codes, F.col(\"iso_country\") == F.col(\"code_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [iso_country#2980], [code_2#3000], Inner\n",
      "   :- Sort [iso_country#2980 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(iso_country#2980, 200), ENSURE_REQUIREMENTS, [plan_id=3731]\n",
      "   :     +- Filter isnotnull(iso_country#2980)\n",
      "   :        +- FileScan parquet [ident#2975,type#2976,name#2977,elevation_ft#2978,continent#2979,iso_country#2980,iso_region#2981,municipality#2982,gps_code#2983,iata_code#2984,local_code#2985,coordinates#2986] Batched: true, DataFilters: [isnotnull(iso_country#2980)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/airport_codes], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n",
      "   +- Sort [code_2#3000 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(code_2#3000, 200), ENSURE_REQUIREMENTS, [plan_id=3732]\n",
      "         +- Filter isnotnull(code_2#3000)\n",
      "            +- FileScan parquet [name#2999,code_2#3000,code_3#3001] Batched: true, DataFilters: [isnotnull(code_2#3000)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/data/country_codes], PartitionFilters: [], PushedFilters: [IsNotNull(code_2)], ReadSchema: struct<name:string,code_2:string,code_3:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+------------+---------+-----------+----------+----------------+--------+---------+----------+-------------------+--------+------+------+\n",
      "|  ident|          type|                name|elevation_ft|continent|iso_country|iso_region|    municipality|gps_code|iata_code|local_code|        coordinates|    name|code_2|code_3|\n",
      "+-------+--------------+--------------------+------------+---------+-----------+----------+----------------+--------+---------+----------+-------------------+--------+------+------+\n",
      "| AD-ALV|      heliport|Andorra la Vella ...|        3450|       EU|         AD|     AD-07|Andorra La Vella|    null|      ALV|      null|1.533551, 42.511174| Andorra|    AD|   AND|\n",
      "|AD-0001|      heliport|      CamÃ­ Heliport|        null|       EU|         AD|     AD-04|      La Massana|    null|     null|      null| 1.51916, 42.546257| Andorra|    AD|   AND|\n",
      "|   TQPF|medium_airport|Clayton J Lloyd I...|         127|       NA|         AI|    AI-U-A|      The Valley|    TQPF|      AXA|      null|-63.055099, 18.2048|Anguilla|    AI|   AIA|\n",
      "+-------+--------------+--------------------+------------+---------+-----------+----------+----------------+--------+---------+----------+-------------------+--------+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
